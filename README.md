# ML-Project
# Sensor Fusion Gesture Classification (CMI/BFRB)

This repository contains a complete deep-learning pipeline for classifying human gestures and behaviors using **multiâ€‘modal sensor data** (IMU + TOF + Thermal). The project includes data preprocessing, feature engineering, model design, K-Fold training, and inference server setup for Kaggle competitions.

---

 ğŸš€ Features

* **Multiâ€‘modal sensor fusion** using IMU + TOF/THM data.
* **Deep CNN + Attention architecture** for timeâ€‘series classification.
* **Separable convolutions** for efficient feature extraction.
* **Squeezeâ€‘andâ€‘Excitation (SE) channel attention**.
* **Temporal Multiâ€‘Head Attention** for sequence modeling.
* **Comprehensive data augmentation** (noise, scaling, shifting, dropout).
* **StandardScaler normalization + lowâ€‘pass filtering** for IMU signals.
* **Stratified Kâ€‘Fold training** with early stopping.
* **Model ensemble inference** for better accuracy.
* **Kaggle-compatible inference server** included.

---

ğŸ“ Project Structure

```
|-- train.py / main.py        # Full pipeline (same as code above)
|-- README.md                 # Project documentation
|-- models/                   # Saved model checkpoints
|-- input/                    # Dataset directory
|   |-- train.csv
|   |-- test.csv
|   |-- test_demographics.csv
|-- artifacts/                # Saved normalizers + metadata
```

---

 ğŸ§  Model Architecture Overview

 1. **IMU Feature Engineering**

* Magnitude features (acc + rotation)
* 1D grouped convolution on accelerometer channels
* Combines raw + engineered features

### 2. **IMU CNN Pathway**

* 2Ã— SeparableConv blocks (Depthwise + Pointwise)
* MaxPooling + SE Attention

### 3. **TOF/THM Pathway**

* Simpler Conv â†’ BN â†’ SiLU â†’ Pool network

### 4. **Fusion + Temporal Attention**

* Concatenate IMU + TOF features
* Multiâ€‘Head Selfâ€‘Attention

### 5. **Classifier Head**

* Global Average Pooling
* Dense layers with BatchNorm + SiLU
* Output logits

---

## ğŸ› ï¸ How to Train

1. Place dataset inside:

```
/input/cmi-detect-behavior-with-sensor-data/
```

2. Ensure `TRAIN_MODE = True` in the config.
3. Run:

```bash
python main.py
```

The script will:

* preprocess and normalize data
* train K-fold models
* save model checkpoints + metadata in the output directory

---

## ğŸ” Inference

1. Set:

```
TRAIN_MODE = False
```

2. Place pretrained model files in:

```
/kaggle/input/cmi-models/
```

3. Run:

```bash
python main.py
```

The inference server will:

* load all folds
* preprocess inputs
* run ensemble prediction
* return gesture label

---

## ğŸ“¦ Artifacts Saved

* `sensor_model_foldX.pth` â€“ trained weights
* `sensor_feature_cols.npy` â€“ feature names
* `behavior_classes.npy` â€“ label encoder classes
* `data_normalizer.pkl` â€“ StandardScaler

---

## ğŸ“Š Data Preprocessing

### Includes:

* Forward/backward fill for missing data
* Low-pass Butterworth filter for IMU
* StandardScaler normalization
* Fixed-length padding/truncation (100 timesteps)

---

## ğŸ§ª Data Augmentation

Applied only in training:

* Gaussian noise on IMU
* Random scaling
* Time shift (roll)
* Nonâ€‘IMU sensor dropout

---

## ğŸ–¥ï¸ Kaggle Inference Compatibility

The project includes:

* Mock server for local testing
* Kaggle CMIInferenceServer integration

---

## âš™ï¸ Requirements

* Python 3.10+
* PyTorch
* Scikit-Learn
* NumPy / Pandas / Polars
* SciPy
* joblib

Install dependencies:

```bash
pip install -r requirements.txt
```

---

## âœ¨ Acknowledgements

This project is inspired by realâ€‘time gesture classification challenges and built with performance, robustness, and clarity in mind.

---

## ğŸ“« Contact

For doubts or improvements, feel free to create an issue or pull request on GitHub.
